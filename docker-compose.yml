version: '3.8'

# Named volumes for DuckDB to avoid filesystem locking issues
# Raw data remains as bind mount for easy data addition from host
volumes:
  duckdb-data:
    external: true
    name: itmx-duckdb-data
  spark-work:
    external: true
    name: itmx-spark-work
  dbt-artifacts:
    external: true
    name: itmx-dbt-artifacts

services:
  # Spark Master
  spark:
    build:
      context: .
      dockerfile: .docker/Dockerfile.spark
    image: itmx-kaggle/spark:latest
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_MASTER_PORT=7077
      - SPARK_NO_DAEMONIZE=true
      - SPARK_MASTER=${SPARK_MASTER}  # From .env
      - DUCKDB_FILENAME=${DUCKDB_FILENAME:-itmx_kaggle.duckdb}
      - KAGGLE_DATA_PATH=/data/raw/kaggle
      - DATA_RAW_PATH=/data/raw
      - DATA_PROCESSED_PATH=/data/processed
      - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY}
      - SPARK_EXECUTOR_CORES=${SPARK_EXECUTOR_CORES}
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    ports:
      - "8081:8080"  # Spark Master UI
      - "7077:7077"  # Spark master port
    volumes:
      - ./spark:/opt/spark/work-dir/spark
      - spark-work:/opt/spark/work
      - duckdb-data:/data/duckdb
      - ./data/raw:/data/raw:ro
      - ./data/processed:/data/processed
    networks:
      - ${DOCKER_NETWORK:-pipeline-network}

  # Spark Worker
  spark-worker:
    image: itmx-kaggle/spark:latest
    container_name: spark-worker-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-2}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-2g}
      - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY}
      - DUCKDB_FILENAME=${DUCKDB_FILENAME:-itmx_kaggle.duckdb}
      - KAGGLE_DATA_PATH=/data/raw/kaggle
      - DATA_RAW_PATH=/data/raw
      - DATA_PROCESSED_PATH=/data/processed
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    ports:
      - "8082:8080"  # Worker UI
    volumes:
      - ./spark:/opt/spark/work-dir/spark
      - spark-work:/opt/spark/work
      - duckdb-data:/data/duckdb
      - ./data/raw:/data/raw:ro
      - ./data/processed:/data/processed
    networks:
      - ${DOCKER_NETWORK:-pipeline-network}

  # Prefect server with custom build (includes all dependencies)
  prefect:
    build:
      context: .
      dockerfile: .docker/Dockerfile.prefect
    image: itmx-kaggle/prefect:latest
    command: prefect server start --host ${PREFECT_SERVER_HOST:-0.0.0.0}
    ports:
      - "${PREFECT_SERVER_PORT:-4200}:4200"
    volumes:
      # Application code - bind mounts for development
      - ./flows:/app/flows
      - ./config.py:/app/config.py
      - ./prefect_utils.py:/app/prefect_utils.py
      - ./dbt:/app/dbt
      - ./.env:/app/.env
      # Data volumes - named volume for DuckDB, bind mounts for data
      - duckdb-data:/data/duckdb
      - ./data/raw:/data/raw
      - ./data/processed:/data/processed
      - dbt-artifacts:/app/dbt/target
    environment:
      - PREFECT_API_URL=${PREFECT_API_URL:-http://localhost:4200/api}
      - PREFECT_SERVER_API_HOST=${PREFECT_SERVER_HOST:-0.0.0.0}
      - DUCKDB_FILENAME=${DUCKDB_FILENAME:-itmx_kaggle.duckdb}
      - KAGGLE_DATA_PATH=/data/raw/kaggle
      - DATA_RAW_PATH=/data/raw
      - DATA_PROCESSED_PATH=/data/processed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # DBT specific - enable multi-threading
      - DBT_THREADS=${DBT_THREADS:-4}
    networks:
      - ${DOCKER_NETWORK:-pipeline-network}

networks:
  pipeline-network:
    driver: bridge
    name: ${DOCKER_NETWORK:-pipeline-network}